import re
import time
import requests
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
import base64
import urllib3

# SSL ê²½ê³  ìˆ¨ê¸°ê¸°
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“¦ Sergio Tacchini ì œí’ˆ ë°ì´í„°ë² ì´ìŠ¤ (v6 ìŠ¤í‚¤ë§ˆ ë°˜ì˜)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PRODUCT_DATABASE = {
    # ê¸°ì¡´ ë°ì´í„° ìœ ì§€
    'TWDJ20656': {
        'name': 'W í´ë¼ì‹œì½” ì½”ë“€ë¡œì´ ë‹¤ìš´ì í¼',
        'price': '359,000',
        'colors': ['NAVY', 'BROWN', 'IVORY'],
        'sizes': 'S, M',
        'features': '- í´ë˜ì‹í•˜ê³  ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ì½”ë“€ë¡œì´ ì†Œì¬ì˜ ì—¬ì„± í´ë˜ì‹ ìœˆí„° ë‹¤ìš´\n- í¬ë¡­í•œ ê¸°ì¥ê°ì— ì†Œë§¤ ì‹œë³´ë¦¬ì™€ ë°‘ë‹¨ ìŠ¤íŠ¸ë§ ì ìš©ìœ¼ë¡œ ë°”ëŒ ìœ ì… ì°¨ë‹¨\n- 2WAY ì§€í¼ ì ìš©\n- ì†Œí”„íŠ¸ ì½”ë“€ë¡œì´ ì†Œì¬ë¡œ ì€ì€í•œ ê´‘íƒê³¼ ë¶€ë“œëŸ¬ìš´ í„°ì¹˜\n- ë•ë‹¤ìš´ ì¶©ì „ì¬ë¡œ ê°€ë²¼ìš´ ì°©ìš©ê°ê³¼ ìš°ìˆ˜í•œ ë³´ì˜¨ë ¥\n- RELAX FIT'
    },
    'TWDJ20156': { 'name': 'W ì¿ ì‰¬ì›œ ë‹¤ìš´ì í¼', 'price': '299,000', 'colors': ['BROWN', 'BLACK', 'IVORY'], 'sizes': 'S, M', 'features': '- ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆì˜ ì‹œê·¸ë‹ˆì²˜ ì•¡í‹°ë¸Œ í´ë˜ì‹ íŒ¨ë”©\n- ê°€ë³ê³  ìŠ¤í¬í‹°í•œ ì ˆê°œë¼ì¸ì´ íŠ¹ì§•\n- ì‹¤ë£¨ì—£ ì¡°ì ˆì´ ê°€ëŠ¥í•œ ìŠ¤íŠ¸ë§ìœ¼ë¡œ ë³¼ë¥¨ê°ìˆëŠ” í• ì—°ì¶œ ê°€ëŠ¥\n- í•˜ì´ë„¥ ë””ìì¸ìœ¼ë¡œ ê²¨ìš¸ì²  ë°”ëŒ ì°¨ë‹¨\n- ë‚´êµ¬ì„±ì´ ë›°ì–´ë‚œ ê³ ë°€ë„ ë‚˜ì¼ë¡  ì†Œì¬ë¡œ ìƒí™œ ë°©ìˆ˜ ê°€ëŠ¥\n- RELAX FIT' },
    'TWMT19056': { 'name': 'W ì¿ ì‰¬ì›œ í”Œë¦¬ìŠ¤ í•˜í”„ì§‘', 'price': '179,000', 'colors': ['KHAKI', 'BROWN', 'IVORY'], 'sizes': 'S, M', 'features': '- ë¶€ë“œëŸ½ê³  í¬ê·¼í•œ í”Œë¦¬ìŠ¤ ì†Œì¬ì˜ í•˜í”„ì§‘ì—…\n- ì†Œë§¤ì— Thumb Hole ì ìš©ìœ¼ë¡œ í™œë™ì„±ê³¼ ë³´ì˜¨ì„± í–¥ìƒ\n- ë„í†°í•œ ë‘ê»˜ê°ê³¼ ì—¬ìœ ìˆëŠ” ì‹¤ë£¨ì—£ìœ¼ë¡œ ì•„ìš°í„°ë¡œë„ í™œìš© ê°€ëŠ¥\n- OVER FIT' },
    'TWMT64556': { 'name': 'W MC í•˜í”„ì§‘ì—… í”Œë¦¬ìŠ¤ í’€ì˜¤ë²„', 'price': '179,000', 'colors': ['NAVY'], 'sizes': 'S, M', 'features': '- ì†Œí”„íŠ¸í•œ í„°ì¹˜ê°ê³¼ ë³´ì˜¨ë ¥ì´ ìš°ìˆ˜í•˜ê³  ê°€ë²¼ìš´ ì¤‘ëŸ‰ê°ì´ íŠ¹ì§•ì¸ ë§ˆì´í¬ë¡œ í”Œë¦¬ìŠ¤ í•˜í”„ì§‘ì—… í’€ì˜¤ë²„\n- ë“±íŒì— ê·¸ë¼ë°ì´ì…˜ ììˆ˜ê¸°ë²•ì„ ì‚¬ìš©í•œ ë¡œê³  ì•„íŠ¸ì› í¬ì¸íŠ¸\n- RELAX FIT' },
    'TWMT64546': { 'name': 'W MC í•˜í”„ì§‘ì—… í”Œë¦¬ìŠ¤ í’€ì˜¤ë²„', 'price': '179,000', 'colors': ['BROWN'], 'sizes': 'S, M', 'features': '- ì†Œí”„íŠ¸í•œ í„°ì¹˜ê°ê³¼ ë³´ì˜¨ë ¥ì´ ìš°ìˆ˜í•˜ê³  ê°€ë²¼ìš´ ì¤‘ëŸ‰ê°ì´ íŠ¹ì§•ì¸ ë§ˆì´í¬ë¡œ í”Œë¦¬ìŠ¤ í•˜í”„ì§‘ì—… í’€ì˜¤ë²„\n- ë“±íŒì— ê·¸ë¼ë°ì´ì…˜ ììˆ˜ê¸°ë²•ì„ ì‚¬ìš©í•œ ë¡œê³  ì•„íŠ¸ì› í¬ì¸íŠ¸\n- RELAX FIT' },
    'TWTR19156': { 'name': 'W ì¿ ì‰¬ì›œ í”Œë¦¬ìŠ¤ í›„ë“œì§‘ì—…', 'price': '179,000', 'colors': ['IVORY', 'KHAKI'], 'sizes': 'S, M', 'features': '- ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆì˜ ì‹œê·¸ë‹ˆì²˜ ì¿ ì‰¬ì›œ ì†Œì¬ì˜ í”Œë¦¬ìŠ¤ ë²„ì „\n- ë°‘ë‹¨ì— ìŠ¤íŠ¸ë§ì„ ì ìš©í•˜ì—¬ ë³¼ë¥¨ê° ìˆëŠ” ì‹¤ë£¨ì—£ ì—°ì¶œ ê°€ëŠ¥\n- í”Œë¦¬ìŠ¤ ì†Œì¬ê°€ ê°€ë²¼ìš´ ê³µê¸°ì¸µì„ í˜•ì„±í•˜ì—¬ ê²½ëŸ‰í•˜ì§€ë§Œ ë”°ëœ»í•˜ë©° ë†€ë¼ìš´ ì‹ ì¶•ì„±\n- RELAX FIT' },
    'TWSK60156': { 'name': 'W MC ë§ˆì´í¬ë¡œí”Œë¦¬ìŠ¤ ìŠ¤ì»¤íŠ¸', 'price': '159,000', 'colors': ['NAVY'], 'sizes': 'S, M', 'features': '- ì†Œí”„íŠ¸í•œ í„°ì¹˜ê°ê³¼ ë³´ì˜¨ë ¥ì´ ìš°ìˆ˜í•˜ê³  ê°€ë²¼ìš´ ì¤‘ëŸ‰ê°ì´ íŠ¹ì§•ì¸ ë§ˆì´í¬ë¡œ í”Œë¦¬ìŠ¤ Aë¼ì¸ ìŠ¤ì»¤íŠ¸\n- í—ˆë¦¬ì— E-ë°´ë“œê°€ ë‚´ì¥ë˜ì–´ í¸ì•ˆí•œ ì°©ìš©\n- ë³¼ ìˆ˜ë‚©ì´ ê°€ëŠ¥í•œ ê¸°ëŠ¥ì„± í¬ì¼“ ì´ë„ˆíŒ¬ì¸  ë‚´ì¥' },
    'TWSK60146': { 'name': 'W MC ë§ˆì´í¬ë¡œí”Œë¦¬ìŠ¤ ìŠ¤ì»¤íŠ¸', 'price': '159,000', 'colors': ['BROWN'], 'sizes': 'S, M', 'features': '- ì†Œí”„íŠ¸í•œ í„°ì¹˜ê°ê³¼ ë³´ì˜¨ë ¥ì´ ìš°ìˆ˜í•˜ê³  ê°€ë²¼ìš´ ì¤‘ëŸ‰ê°ì´ íŠ¹ì§•ì¸ ë§ˆì´í¬ë¡œ í”Œë¦¬ìŠ¤ Aë¼ì¸ ìŠ¤ì»¤íŠ¸\n- í—ˆë¦¬ì— E-ë°´ë“œê°€ ë‚´ì¥ë˜ì–´ í¸ì•ˆí•œ ì°©ìš©\n- ë³¼ ìˆ˜ë‚©ì´ ê°€ëŠ¥í•œ ê¸°ëŠ¥ì„± í¬ì¼“ ì´ë„ˆíŒ¬ì¸  ë‚´ì¥' },
    'TWPT19156': { 'name': 'W ì¿ ì‰¬ì›œ í”Œë¦¬ìŠ¤ ë£¨ì¦ˆ ì¡°ê±° íŒ¬ì¸ ', 'price': '149,000', 'colors': ['IVORY', 'KHAKI'], 'sizes': 'S, M', 'features': '- ë¶€ë“œëŸ½ê³  í¬ê·¼í•œ í”Œë¦¬ìŠ¤ ì†Œì¬ì˜ ì¡°ê±° íŒ¬ì¸ \n- ë„‰ë„‰í•œ ë£¨ì¦ˆí•ìœ¼ë¡œ í¸ì•ˆí•œ ì°©ìš©ê°\n- ë°‘ë‹¨ ì‹œë³´ë¦¬ë¡œ í• ì¡°ì ˆ ê°€ëŠ¥' },
    'TWPT10844': { 'name': 'W ì—ì„¼ì…œ ë¶€ì¸ ì»· ë ˆê¹…ìŠ¤ íŒ¬ì¸ ', 'price': '129,000', 'colors': ['BLACK', 'NAVY', 'BROWN'], 'sizes': 'S, M', 'features': '- ê²¨ìš¸ ì‹œì¦Œì— ê°€ì¥ í™œìš©ë„ ë†’ê²Œ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ì½”íŠ¸ ë ˆê¹…ìŠ¤ íŒ¬ì¸ \n- ë¶€ì¸ ì»· ì‹¤ë£¨ì—£ìœ¼ë¡œ ë‹¤ë¦¬ ë¼ì¸ì„ ê¸¸ì–´ë³´ì´ê²Œ ì—°ì¶œ' },
    'TWPT11046': { 'name': 'W ì—ì„¼ì…œ ê¸°ëª¨ ë¶€ì¸ ì»· ë ˆê¹…ìŠ¤ íŒ¬ì¸ ', 'price': '129,000', 'colors': ['BLACK', 'BROWN'], 'sizes': 'S, M', 'features': '- ë”°ëœ»í•œ ê¸°ëª¨ ì•ˆê°ì´ ì ìš©ëœ ë¶€ì¸ ì»· ë ˆê¹…ìŠ¤ íŒ¬ì¸ \n- ê²¨ìš¸ì²  ë³´ì˜¨ì„±ê³¼ ìŠ¤íƒ€ì¼ì„ ë™ì‹œì—' },
    'TXMT16054': { 'name': 'U ì¿ ì‰¬ë¼ì´íŠ¸ í”„ë Œì¹˜ í´ë˜ì‹ ë§¨íˆ¬ë§¨', 'price': '119,000', 'colors': ['NAVY', 'IVORY', 'BLACK'], 'sizes': 'S, M, L', 'features': '- í”„ë Œì¹˜ í…Œë¦¬ ì†Œì¬ì˜ í´ë˜ì‹ ë§¨íˆ¬ë§¨\n- ê°€ë³ê³  ë¶€ë“œëŸ¬ìš´ ì°©ìš©ê°\n- ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆ ì‹œê·¸ë‹ˆì²˜ ë¡œê³  ììˆ˜' },
    'TXMT14054': { 'name': 'U ì¿ ì‰¬ë¼ì´íŠ¸ ë² ì´ì§ ë§¨íˆ¬ë§¨', 'price': '119,000', 'colors': ['IVORY', 'BLACK', 'NAVY'], 'sizes': 'S, M, L', 'features': '- ë² ì´ì§í•œ ë””ìì¸ì˜ ë°ì¼ë¦¬ ë§¨íˆ¬ë§¨\n- ê°€ë³ê³  ë¶€ë“œëŸ¬ìš´ ì¿ ì‰¬ë¼ì´íŠ¸ ì†Œì¬' },
    'TXMT14154': { 'name': 'U ì¿ ì‰¬ë¼ì´íŠ¸ í•˜í”„ì§‘', 'price': '129,000', 'colors': ['BLACK', 'IVORY'], 'sizes': 'S, M, L', 'features': '- ë² ì´ì§í•œ ë””ìì¸ì˜ í•˜í”„ì§‘ì—…\n- ê°€ë³ê³  ë¶€ë“œëŸ¬ìš´ ì¿ ì‰¬ë¼ì´íŠ¸ ì†Œì¬' },
    'TMMT15056': { 'name': 'M í´ë˜ì‹ ê¸°ëª¨ í•˜í”„ì§‘ í’€ì˜¤ë²„', 'price': '139,000', 'colors': ['BLACK', 'NAVY', 'IVORY'], 'sizes': 'M, L, XL', 'features': '- ë”°ëœ»í•œ ê¸°ëª¨ ì•ˆê°ì´ ì ìš©ëœ í•˜í”„ì§‘ í’€ì˜¤ë²„\n- í´ë˜ì‹í•œ ë””ìì¸ê³¼ í¸ì•ˆí•œ ì°©ìš©ê°' },
    'TXSO4105N': { 'name': 'U 3-PACK í¬ë£¨ ì‚­ìŠ¤', 'price': '19,900', 'colors': ['WHITE', 'BLACK', 'MIXED'], 'sizes': 'FREE', 'features': '- ë°ì¼ë¦¬ë¡œ í™œìš©í•˜ê¸° ì¢‹ì€ í¬ë£¨ ì‚­ìŠ¤ 3íŒ© ì„¸íŠ¸\n- ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆ ë¡œê³  í¬ì¸íŠ¸' },
    'TWSO9044N': { 'name': 'W ì…”ë§ ì˜¤ë²„ ë‹ˆì‚­ìŠ¤', 'price': '35,000', 'colors': ['BLACK', 'WHITE'], 'sizes': 'FREE', 'features': '- ì…”ë§ ë””í…Œì¼ì´ í¬ì¸íŠ¸ì¸ ì˜¤ë²„ ë‹ˆì‚­ìŠ¤\n- í…Œë‹ˆìŠ¤ ìŠ¤ì»¤íŠ¸ì™€ ë§¤ì¹˜í•˜ê¸° ì¢‹ì€ ì•„ì´í…œ' },
    'TXSO4015N': { 'name': 'U ì—ì„¼ì…œ í¬ë£¨ ì‚­ìŠ¤', 'price': '15,000', 'colors': ['WHITE', 'BLACK'], 'sizes': 'FREE', 'features': '- ì—ì„¼ì…œ ë¼ì¸ì˜ ë² ì´ì§ í¬ë£¨ ì‚­ìŠ¤\n- ë°ì¼ë¦¬ë¡œ í™œìš©í•˜ê¸° ì¢‹ì€ ì•„ì´í…œ' }
}

def extract_product_code(input_url):
    # URL ì •ì œ (íŒŒë¼ë¯¸í„° ì œê±°)
    clean_url = input_url.split('?')[0]
    
    # ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆ íŒ¨í„´
    sergio_patterns = [
        r'sergiotacchini\.co\.kr\/product-detail\/([A-Z0-9]+)-[A-Z]{2,3}',
        r'product-detail\/([A-Z0-9]+)-[A-Z]{2,3}',
        r'([A-Z]{2,4}\d{5})-[A-Z]{2,3}',
        r'([A-Z]{4}\d{5}[A-Z]?)'
    ]
    
    # ë“€ë² í‹°ì¹´ íŒ¨í„´ (ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆì™€ ë™ì¼í•œ URL êµ¬ì¡°)
    duvetica_patterns = [
        r'duvetica\.co\.kr\/product-detail\/([A-Z0-9]+)-[A-Z]{2,3}',
        r'duvetica\.co\.kr\/product\/([^\/\?]+)',
        r'duvetica\.co\.kr\/goods\/([^\/\?]+)',
        r'\/product\/([A-Z0-9\-]+)',
        r'\/goods\/view\?no=(\d+)',
        r'goodsNo=(\d+)'
    ]
    
    # 1ì°¨ ì‹œë„: ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆ íŒ¨í„´
    for pattern in sergio_patterns:
        match = re.search(pattern, clean_url, re.IGNORECASE)
        if match:
            return match.group(1).upper()
    
    # 2ì°¨ ì‹œë„: ë“€ë² í‹°ì¹´ íŒ¨í„´
    for pattern in duvetica_patterns:
        match = re.search(pattern, clean_url, re.IGNORECASE)
        if match:
            return match.group(1)
            
    # 3ì°¨ ì‹œë„: ì›ë³¸ URLë¡œ í™•ì¸ (í˜¹ì‹œ íŒŒë¼ë¯¸í„° ìª½ì— ì½”ë“œê°€ ìˆì„ ê²½ìš° ëŒ€ë¹„)
    for pattern in sergio_patterns + duvetica_patterns:
        match = re.search(pattern, input_url, re.IGNORECASE)
        if match:
            return match.group(1)
            
    # ì§ì ‘ ì…ë ¥ íŒ¨í„´
    direct_match = re.match(r'^([A-Z]{4}\d{5}[A-Z]?)(?:-[A-Z]{2,3})?$', input_url.strip().upper())
    if direct_match:
        return direct_match.group(1)
        
    return None

def get_referer_from_url(url):
    """URLì—ì„œ Referer ì¶”ì¶œ"""
    if 'duvetica' in url.lower():
        return 'https://www.duvetica.co.kr/'
    elif 'sergiotacchini' in url.lower():
        return 'https://www.sergiotacchini.co.kr/'
    else:
        # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        match = re.search(r'https?://([^/]+)', url)
        if match:
            return f"https://{match.group(1)}/"
        return ''

def process_image_from_url(img_url, referer=None):
    """ì´ë¯¸ì§€ URLì—ì„œ ì¸ë„¤ì¼ ìƒì„±"""
    try:
        if not referer:
            referer = get_referer_from_url(img_url)
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': referer
        }
        # verify=Falseë¡œ SSL ì—ëŸ¬ ë°©ì§€
        img_response = requests.get(img_url, headers=headers, timeout=10, verify=False)
        if img_response.status_code != 200:
            return None
        img = Image.open(BytesIO(img_response.content))
        
        # íˆ¬ëª… ë°°ê²½ì„ í°ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬
        if img.mode in ("RGBA", "P", "LA"):
            # í°ìƒ‰ ë°°ê²½ ì´ë¯¸ì§€ ìƒì„±
            background = Image.new("RGB", img.size, (255, 255, 255))
            if img.mode == "P":
                img = img.convert("RGBA")
            # ì•ŒíŒŒ ì±„ë„ì´ ìˆìœ¼ë©´ í•©ì„±
            if img.mode in ("RGBA", "LA"):
                background.paste(img, mask=img.split()[-1])  # ì•ŒíŒŒ ì±„ë„ì„ ë§ˆìŠ¤í¬ë¡œ ì‚¬ìš©
            else:
                background.paste(img)
            img = background
        elif img.mode != "RGB":
            img = img.convert("RGB")
            
        img.thumbnail((300, 300))
        buffered = BytesIO()
        img.save(buffered, format="JPEG", quality=85)
        img_str = base64.b64encode(buffered.getvalue()).decode()
        return f"data:image/jpeg;base64,{img_str}"
    except Exception as e:
        print(f"Image process failed: {e}")
        return None

def crawl_product_page(url, product_code):
    """
    DBì— ì—†ëŠ” ì œí’ˆì¼ ê²½ìš°, ì‹¤ì œ ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ì„¸ë¥´ì§€ì˜¤ íƒ€í‚¤ë‹ˆ, ë“€ë² í‹°ì¹´ ë“± ë‹¤ì–‘í•œ ì‡¼í•‘ëª°ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    try:
        import json as json_module
        
        referer = get_referer_from_url(url)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Referer': referer
        }
        # verify=False ì˜µì…˜ ì¶”ê°€
        response = requests.get(url, headers=headers, timeout=10, verify=False)
        if response.status_code != 200:
            print(f"Failed to connect: {response.status_code}")
            return None
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # __NEXT_DATA__ ì—ì„œ ë°ì´í„° ì¶”ì¶œ ì‹œë„ (ì„¸ë¥´ì§€ì˜¤íƒ€í‚¤ë‹ˆ, ë“€ë² í‹°ì¹´ ë“± Next.js ì‚¬ì´íŠ¸)
        next_data_script = soup.find('script', id='__NEXT_DATA__')
        next_data = None
        if next_data_script and next_data_script.string:
            try:
                next_data = json_module.loads(next_data_script.string)
                next_data = next_data.get('props', {}).get('pageProps', {}).get('data', {})
            except:
                next_data = None
        
        # 1. ì œí’ˆëª… ì¶”ì¶œ
        name = ""
        # __NEXT_DATA__ì—ì„œ ì¶”ì¶œ
        if next_data:
            name_data = next_data.get('name', {})
            if isinstance(name_data, dict):
                name = name_data.get('pc', '') or name_data.get('mobile', '')
            elif isinstance(name_data, str):
                name = name_data
        
        # og:title ì‹œë„
        if not name:
            og_title = soup.find('meta', property='og:title')
            if og_title and og_title.get('content'):
                name = og_title['content']
        
        # h1 íƒœê·¸ ì‹œë„
        if not name:
            h1 = soup.find('h1')
            if h1:
                name = h1.get_text().strip()
        
        # ìƒí’ˆëª… í´ë˜ìŠ¤ ì‹œë„ (ë‹¤ì–‘í•œ ì‡¼í•‘ëª° ëŒ€ì‘)
        if not name:
            for selector in ['.goods_name', '.product-name', '.prd_name', '.item_name', '[class*="product"][class*="name"]']:
                elem = soup.select_one(selector)
                if elem:
                    name = elem.get_text().strip()
                    break
        
        if name:
            name = name.split(' - ')[0].split(' | ')[0].strip()
        
        # 2. ê°€ê²© ì¶”ì¶œ
        price = ""
        # __NEXT_DATA__ì—ì„œ ì¶”ì¶œ
        if next_data:
            price_data = next_data.get('price', {})
            if isinstance(price_data, dict):
                price_val = price_data.get('discounted') or price_data.get('original')
                if price_val:
                    price = f"{price_val:,}"
        
        # og:descriptionì—ì„œ ê°€ê²© ì°¾ê¸°
        if not price:
            og_desc = soup.find('meta', property='og:description')
            if og_desc and og_desc.get('content'):
                price_match = re.search(r'(\d{1,3}(?:,\d{3})+)ì›?', og_desc['content'])
                if price_match:
                    price = price_match.group(1)
        
        # ê°€ê²© í´ë˜ìŠ¤ì—ì„œ ì°¾ê¸°
        if not price:
            for selector in ['.price', '.goods_price', '.prd_price', '.sell_price', '[class*="price"]']:
                elem = soup.select_one(selector)
                if elem:
                    price_match = re.search(r'(\d{1,3}(?:,\d{3})+)', elem.get_text())
                    if price_match:
                        price = price_match.group(1)
                        break
        
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© ì°¾ê¸°
        if not price:
            text_content = soup.get_text()
            price_matches = re.findall(r'(\d{1,3}(?:,\d{3})+)', text_content)
            for p in price_matches:
                val = int(p.replace(',', ''))
                if 10000 < val < 5000000: 
                    price = p
                    break
                    
        # 3. ì»¬ëŸ¬ ì¶”ì¶œ (__NEXT_DATA__ ìš°ì„ )
        colors = []
        
        # __NEXT_DATA__ì—ì„œ ì»¬ëŸ¬ ì¶”ì¶œ
        if next_data:
            color_options = next_data.get('colorOptions', [])
            for opt in color_options:
                color_name = opt.get('name', '')
                if color_name and color_name not in colors:
                    colors.append(color_name)
        
        # ë°©ë²• 1: ì»¬ëŸ¬ ê´€ë ¨ select/optionì—ì„œ ì¶”ì¶œ
        if not colors:
            color_selectors = [
                'select[name*="color"] option',
                'select[name*="Color"] option',
                'select[id*="color"] option',
                '[class*="color"] option',
                '[class*="Color"] option'
            ]
            for selector in color_selectors:
                options = soup.select(selector)
                for opt in options:
                    text = opt.get_text().strip()
                    value = opt.get('value', '')
                    if text and text not in ['ì„ íƒ', 'ì»¬ëŸ¬ì„ íƒ', 'ìƒ‰ìƒì„ íƒ', '- ì„ íƒ -', '']:
                        colors.append(text)
                if colors:
                    break
        
        # ë°©ë²• 2: ì»¬ëŸ¬ ë²„íŠ¼/ë¼ë²¨ì—ì„œ ì¶”ì¶œ
        if not colors:
            color_elements = soup.select('[class*="color"] li, [class*="color"] span, [class*="color"] label, [class*="Color"] li')
            for elem in color_elements:
                text = elem.get_text().strip()
                title = elem.get('title', '')
                if title and len(title) < 20:
                    colors.append(title)
                elif text and len(text) < 20 and text not in colors:
                    colors.append(text)
        
        # ë°©ë²• 3: data ì†ì„±ì—ì„œ ì¶”ì¶œ
        if not colors:
            color_data = soup.select('[data-color], [data-option-color]')
            for elem in color_data:
                color_val = elem.get('data-color') or elem.get('data-option-color')
                if color_val and color_val not in colors:
                    colors.append(color_val)
        
        # ë°©ë²• 4: í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ COLOR/ì»¬ëŸ¬ ì„¹ì…˜ ì°¾ê¸°
        if not colors:
            text_content = soup.get_text()
            color_match = re.search(r'(?:COLOR|ì»¬ëŸ¬|ìƒ‰ìƒ)\s*[:\-]?\s*([A-Zê°€-í£\s,/]+?)(?:\n|SIZE|ì‚¬ì´ì¦ˆ|$)', text_content, re.IGNORECASE)
            if color_match:
                color_str = color_match.group(1).strip()
                colors = [c.strip() for c in re.split(r'[,/]', color_str) if c.strip() and len(c.strip()) < 15]
        
        colors_str = ", ".join(colors[:5]) if colors else ""  # ìµœëŒ€ 5ê°œ
        
        # 4. ì‚¬ì´ì¦ˆ ì¶”ì¶œ (__NEXT_DATA__ ìš°ì„ )
        sizes = []
        
        # __NEXT_DATA__ì—ì„œ ì‚¬ì´ì¦ˆ ì¶”ì¶œ
        if next_data:
            item_options = next_data.get('itemOptions', [])
            for opt in item_options:
                size_name = opt.get('size', '')
                if size_name and size_name.upper() not in [s.upper() for s in sizes]:
                    sizes.append(size_name.upper())
        
        # ë°©ë²• 1: ì‚¬ì´ì¦ˆ ê´€ë ¨ select/optionì—ì„œ ì¶”ì¶œ
        if not sizes:
            size_selectors = [
                'select[name*="size"] option',
                'select[name*="Size"] option',
                'select[id*="size"] option',
                '[class*="size"] option',
                '[class*="Size"] option'
            ]
            for selector in size_selectors:
                options = soup.select(selector)
                for opt in options:
                    text = opt.get_text().strip()
                    if text and text not in ['ì„ íƒ', 'ì‚¬ì´ì¦ˆì„ íƒ', '- ì„ íƒ -', '']:
                        # ì‚¬ì´ì¦ˆ í˜•ì‹ ì²´í¬ (S, M, L, XL, ìˆ«ì ë“±)
                        if re.match(r'^(XS|S|M|L|XL|XXL|FREE|\d+)$', text, re.IGNORECASE):
                            sizes.append(text.upper())
                if sizes:
                    break
        
        # ë°©ë²• 2: ì‚¬ì´ì¦ˆ ë²„íŠ¼/ë¼ë²¨ì—ì„œ ì¶”ì¶œ
        if not sizes:
            size_elements = soup.select('[class*="size"] li, [class*="size"] span, [class*="size"] label, [class*="Size"] li')
            for elem in size_elements:
                text = elem.get_text().strip()
                if re.match(r'^(XS|S|M|L|XL|XXL|FREE|\d+)$', text, re.IGNORECASE):
                    sizes.append(text.upper())
        
        # ë°©ë²• 3: data ì†ì„±ì—ì„œ ì¶”ì¶œ
        if not sizes:
            size_data = soup.select('[data-size], [data-option-size]')
            for elem in size_data:
                size_val = elem.get('data-size') or elem.get('data-option-size')
                if size_val and size_val.upper() not in sizes:
                    sizes.append(size_val.upper())
        
        # ë°©ë²• 4: í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ SIZE ì„¹ì…˜ ì°¾ê¸°
        if not sizes:
            text_content = soup.get_text()
            size_match = re.search(r'(?:SIZE|ì‚¬ì´ì¦ˆ)\s*[:\-]?\s*([A-Z0-9\s,/]+?)(?:\n|COLOR|ì»¬ëŸ¬|$)', text_content, re.IGNORECASE)
            if size_match:
                size_str = size_match.group(1).strip()
                for s in re.split(r'[,/\s]+', size_str):
                    s = s.strip().upper()
                    if s and re.match(r'^(XS|S|M|L|XL|XXL|FREE|\d+)$', s):
                        sizes.append(s)
        
        sizes_str = ", ".join(sorted(set(sizes), key=lambda x: ['XS','S','M','L','XL','XXL','FREE'].index(x) if x in ['XS','S','M','L','XL','XXL','FREE'] else 99)) if sizes else ""
        
        # 5. íŠ¹ì§• ì¶”ì¶œ (ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„)
        features = ""
        
        # DESCRIPTION ì˜ì—­ ì°¾ê¸°
        desc_markers = soup.find_all(string=re.compile("DESCRIPTION|ìƒí’ˆì„¤ëª…|ì œí’ˆì„¤ëª…|ìƒì„¸ì •ë³´", re.IGNORECASE))
        for marker in desc_markers:
            parent_section = marker.find_parent('div') or marker.find_parent('section')
            if parent_section:
                text = parent_section.get_text(separator='\n')
                for keyword in ["DESCRIPTION", "ìƒí’ˆì„¤ëª…", "ì œí’ˆì„¤ëª…", "ìƒì„¸ì •ë³´"]:
                    if keyword in text:
                        parts = text.split(keyword)
                        target_text = parts[1] if len(parts) > 1 else text
                        for stopper in ["ìƒí’ˆì½”ë“œ", "ì†Œì¬", "ì œì¡°ë…„ì›”", "SIZE", "SHIPPING", "ë°°ì†¡", "êµí™˜", "ë°˜í’ˆ"]:
                            if stopper in target_text:
                                target_text = target_text.split(stopper)[0]
                        lines = [line.strip() for line in target_text.split('\n') if line.strip()]
                        clean_lines = []
                        for line in lines:
                            if len(line) > 2 and len(line) < 200:
                                if not line.startswith('-') and not line.startswith('*'):
                                    clean_lines.append(f"- {line}")
                                else:
                                    clean_lines.append(line)
                        features = "\n".join(clean_lines[:6])
                        if features:
                            break
                if features:
                    break

        # 6. ì´ë¯¸ì§€ ì¶”ì¶œ
        og_image = soup.find('meta', property='og:image')
        image_url = og_image['content'] if og_image and og_image.get('content') else None
        
        # og:imageê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if not image_url:
            for selector in ['.goods_image img', '.product-image img', '.prd_img img', '[class*="product"] img']:
                elem = soup.select_one(selector)
                if elem and elem.get('src'):
                    image_url = elem['src']
                    break
        
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if image_url and not image_url.startswith('http'):
            # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
            domain_match = re.search(r'(https?://[^/]+)', url)
            if domain_match:
                image_url = f"{domain_match.group(1)}{image_url}"
            
        processed_image = process_image_from_url(image_url, referer) if image_url else None
        if not processed_image:
            processed_image = f"https://placehold.co/300x300/png?text={product_code}"

        return {
            "id": int(time.time() * 1000),
            "name": name,
            "price": price,
            "colors": colors_str, 
            "sizes": sizes_str, 
            "features": features,
            "productCode": product_code,
            "productUrl": url,
            "imageUrl": processed_image,
            "isMain": False
        }

    except Exception as e:
        print(f"Crawling failed: {e}")
        return None

def fetch_product_info(url_input):
    if not url_input:
        return None
        
    product_code = extract_product_code(url_input)
    if not product_code:
        product_code = "UNKNOWN"
    
    target_url = url_input.split('?')[0] if "http" in url_input else ""
    
    # 1. DB ì¡°íšŒ
    if product_code in PRODUCT_DATABASE:
        info = PRODUCT_DATABASE[product_code]
        if not target_url:
            first_color_suffix = f"{info['colors'][0][:2]}S" if info['colors'] else "BKS"
            target_url = f"https://www.sergiotacchini.co.kr/product-detail/{product_code}-{first_color_suffix}"
            
        thumbnail_data = None
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            res = requests.get(target_url, headers=headers, timeout=5, verify=False)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                og_image = soup.find('meta', property='og:image')
                if og_image:
                    thumbnail_data = process_image_from_url(og_image['content'])
        except:
            pass
            
        if not thumbnail_data:
            thumbnail_data = f"https://placehold.co/300x300/png?text={product_code}"
        
        return {
            "id": int(time.time() * 1000),
            "name": info['name'],
            "price": info['price'],
            "colors": ", ".join(info['colors']),
            "sizes": info['sizes'],
            "features": info['features'],
            "productCode": product_code,
            "productUrl": target_url,
            "imageUrl": thumbnail_data,
            "isMain": False
        }
    
    # 2. í¬ë¡¤ë§ ì‹œë„
    if target_url:
        crawled_data = crawl_product_page(target_url, product_code)
        if crawled_data:
            return crawled_data
            
    # 3. ì‹¤íŒ¨ ì‹œ ë¹ˆ í¼ì´ë¼ë„ ë°˜í™˜ (ì—ëŸ¬ ë°©ì§€)
    return {
        "id": int(time.time() * 1000),
        "name": "",
        "price": "",
        "colors": "",
        "sizes": "",
        "features": "",
        "productCode": product_code,
        "productUrl": target_url or url_input,
        "imageUrl": f"https://placehold.co/300x300/png?text={product_code}",
        "isMain": False
    }
